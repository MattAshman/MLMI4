{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl, sys\n",
    "\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "sys.path.append('../code/')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "from pathlib import Path\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import SquaredExponential, White\n",
    "from gpflow.optimizers import Scipy\n",
    "from gpflow.utilities import print_summary, triangular\n",
    "from gpflow.base import Parameter\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from original_datasets import Datasets\n",
    "from dgp import DGP\n",
    "\n",
    "output_logdir = '/tmp/tensorboard'\n",
    "\n",
    "!rm -rf '{output_logdir}'\n",
    "!mkdir '{output_logdir}'\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "def enumerated_logdir(_logdir_id: int = [0]):\n",
    "    logdir = Path(output_logdir, str(_logdir_id[0]))\n",
    "    _logdir_id[0] += 1\n",
    "    return str(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Datasets(data_path='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.all_datasets['concrete'].get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = kmeans2(X, 50, minit='points')[0]\n",
    "D = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = []\n",
    "for l in range(3):\n",
    "    kernels.append(SquaredExponential() + White(variance=2e-6))\n",
    "    \n",
    "model = DGP(D, kernels, Gaussian(variance=0.05), Z, \n",
    "            num_outputs=Y.shape[1], num_samples=1)\n",
    "\n",
    "# start inner layers deterministically\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.q_sqrt = Parameter(layer.q_sqrt.value() * 1e-5, \n",
    "                             transform=triangular())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.optimizers.Adam(0.01)\n",
    "\n",
    "def optimisation_step(model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        obj = - model.elbo(X, Y, full_cov=False)\n",
    "        grad = tape.gradient(obj, model.trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grad, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitored_training_loop(model, logdir,\n",
    "                            epochs=1, logging_epoch_freq=10):\n",
    "    summary_writer = tf.summary.create_file_writer(logdir)\n",
    "    tf_optimisation_step = tf.function(optimisation_step)\n",
    "#     tf_optimisation_step = optimisation_step\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            tf_optimisation_step(model)\n",
    "                \n",
    "            epoch_id = epoch + 1\n",
    "            if epoch_id % logging_epoch_freq == 0:\n",
    "                tf.print(f'Epoch {epoch_id}: ELBO (train) {model.elbo(X, Y)}')\n",
    "                \n",
    "                # computes the mean and variance of the help-out data at \n",
    "                # input points\n",
    "#                 mean, var = model.predict_f_full_cov(Xs, num_samples=10)\n",
    "                # produces samples from the posterior at input points\n",
    "#                 samples, _, _ = model.predict_all_layers_full_cov(Xs, num_samples=10)\n",
    "#                 fig = plotting_regression(X, Y, Xs, mean, var, samples)\n",
    "                \n",
    "#                 summary_matplotlib_image(dict(model_samples=fig), step=epoch)\n",
    "                tf.summary.scalar('elbo', data=model.elbo(X, Y), step=epoch)\n",
    "                tf.summary.scalar('likelihood/variance', data=model.likelihood.variance, step=epoch)\n",
    "                tf.summary.scalar('layer1_kernel/lengthscale', data=model.layers[0].kernel.kernels[0].lengthscale, step=epoch)\n",
    "                tf.summary.scalar('layer1_kernel/variance', data=model.layers[0].kernel.kernels[0].variance, step=epoch)\n",
    "                tf.summary.scalar('layer2_kernel/lengthscale', data=model.layers[1].kernel.kernels[0].lengthscale, step=epoch)\n",
    "                tf.summary.scalar('layer2_kernel/variance', data=model.layers[1].kernel.kernels[0].variance, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: ELBO (train) -1015.1031979066487\n",
      "Epoch 400: ELBO (train) -833.2478216386254\n",
      "Epoch 600: ELBO (train) -706.7946142692124\n",
      "Epoch 800: ELBO (train) -649.4962408972034\n",
      "Epoch 1000: ELBO (train) -633.1867308808069\n",
      "Epoch 1200: ELBO (train) -610.3892083505866\n",
      "Epoch 1400: ELBO (train) -585.285414135895\n",
      "Epoch 1600: ELBO (train) -608.3614653709841\n",
      "Epoch 1800: ELBO (train) -574.45752432212\n",
      "Epoch 2000: ELBO (train) -582.2338566867653\n",
      "Epoch 2200: ELBO (train) -562.1402554650483\n",
      "Epoch 2400: ELBO (train) -568.5105722570199\n",
      "Epoch 2600: ELBO (train) -580.9915916326896\n",
      "Epoch 2800: ELBO (train) -557.8225633817202\n",
      "Epoch 3000: ELBO (train) -550.3971204938057\n",
      "Epoch 3200: ELBO (train) -557.0845845893652\n",
      "Epoch 3400: ELBO (train) -551.9559737578347\n",
      "Epoch 3600: ELBO (train) -544.3743208824665\n",
      "Epoch 3800: ELBO (train) -537.3167615794084\n",
      "Epoch 4000: ELBO (train) -559.1180490333746\n",
      "Epoch 4200: ELBO (train) -544.669017419691\n",
      "Epoch 4400: ELBO (train) -545.7962400503319\n",
      "Epoch 4600: ELBO (train) -567.4895833731136\n",
      "Epoch 4800: ELBO (train) -563.2320725336746\n",
      "Epoch 5000: ELBO (train) -565.9670274044252\n"
     ]
    }
   ],
   "source": [
    "monitored_training_loop(model, logdir=enumerated_logdir(), \n",
    "                        epochs=5000, logging_epoch_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ll = model.log_likelihood(Xs, Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-3.0104994570009054, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(test_ll / Xs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi4-env",
   "language": "python",
   "name": "mlmi4-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
